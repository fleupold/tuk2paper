%!TEX root = ../document.tex
\section{Implementation Ideas} \label{sec:IMPLEMENTATION_IDEAS}
%Lauritz
Besides providing an unobstrusive user interface, either implemented as a plugin for an existing integrated development environment or else in form of a complete development environment, an implementation of our idea would primarily require components that gather data contexts from actual program runs as well as components that support developers in selecting appropriate and interesting data contexts during development.

\subsection{Gathering Data Contexts}
As described in Section~\ref{sec:PAPER_PROTOTYPE} and \ref{sec:FINAL_PROTOTYPE}, our envisioned environment always presents relevant runtime data when developers interact with statements as, for example, when they click or hover on single lines of source code.
Further, developers should be able to evaluate arbitrary lines, including lines containing database queries, to see and understand the effects of their code on the application as well as the database layer.
That is, our approach relies on providing developers with actual runtime data and real database tables already during development and, therefore, depends on the availability of such data contexts for the presented code base.
Such data contexts need to be completely realistic and immediately available to be useful to developers.
Given these requirements, two approaches could provide data contexts for specific code sections of interest to the developer:
Either, the interactions with particular statements in code trigger deterministic test runs that cover that section or else, tracing the application's execution records data contexts in advance.
Recording data beforehand obviously also could rely on test runs, but could also be captured in deployed systems used by actual customer to provide evee more realistic runtime data.
Further, recorded data contexts need just be fetched from a database, which also could be an in-memory database, and, thus, potentially provides data contexts faster than establishing runtime data through running tests.
The time required to run tests depends on the granularity of the covering tests with a full spectrum from fine-grained, isolated unit tests to high-level, long-running acceptance tests.
However, running tests to emulate runtime during development would only require to store associations between code and tests, while recording in advance implies storing as much data as necessary to actually run any statements in the sources of an application.
A reasonable trade-off between space and time consumption, which we would probably investigate first, could record data not on the level of single statements but on the level of method scopes, when we assume deterministic code execution.
Such an implementation would record all data necessary to actually run a method with all its statement, including provided parameters, accessible state, and the the database at the moment the method is entered, as shown in Figure~\ref{fig:context_recording}.

\begin{figure}
    \includegraphics[width=\linewidth]{images/context_recording}
    \caption{Running the statements of this Java method with its embedded SQL query requires all data highlighted in red: the parameter, accessible state, and a database.}
    \label{fig:context_recording}
\end{figure}.

When a developer then interacts with a specific line in the method's body, we could use ordinary breakpoints to establish the necessary runtime data for that specific line of interest. 
Accessible state includes global state and depending on the used programming language also state from surrounding scopes as, for example, from surrounding functions or the receiver.
Snapshots of the database are necessary as methods that embedd queries might modify the database, potentially impeding subsequent runs of that method during the interactive development.
\subsection{Selecting Data Contexts}
These tracing approaches generates potentially numerous data contexts.
When our tool would trace real deployed systems, it could capture any number of data contexts for a particular scope.
Given our system captures a data context for each method call and a specific method is, for example, called whenever a user clicks a button, the system would capture an arbitrarily high number of data contexts for this situation.
Further, even when our tool only traces test execution on replicated databases, particular scopes might not only be executed in many test cases, but also multiple times in a single test case as, for example, obviously the case with methods called from loop bodies.
For these reasons, a developer using our tool might be confronted with many data contexts for certain situations.
However, as the number of available data contexts might be arbitrarily high and data context from the same trace might also overlap considerably, presenting all data contexts to the developer probably reduces usability significantly. 
Approaches addressing this issue include automatic selection of interesting samples after clustering all data contexts, preselection of interesting data contexts through experts, and combinations of those two methods.
Clustering could be based on at several dimensions, including \todo{some image for the clustering}:
\begin{itemize}
  \item size of the context's database
  \item control flow that lead to this context
  \item execution time of the context's scope
\end{itemize}
Nevertheless, even aided through automatic clustering or manual preselections, our tools should probably also support developers in exploring the full extent of available contexts and make the final decision on which specific data context they want to use during development.